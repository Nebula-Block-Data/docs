
# Overview
Welcome to Nebula Block's Serverless Endpoints for LLMs, the easiest way to integrate powerful large 
language models (LLMs) like Meta LLaMA and Qwen into your applications. With Nebula Block, endpoints 
are pre-configured for you—simply sign up, log in, and start using them right away! You can explore 
the capabilities of our LLMs with a live chat or integrate them into your projects using provided code 
samples in curl, Python, or JavaScript.

## Key Features
- **Pre-Created Endpoints:** No setup required—get started instantly after signing in.
- **Live Chat:** Test models interactively with a chat interface.
- **Code Samples:** Quick-start guides for curl, Python, and JavaScript.
- **Support for Popular Models:** Choose from a range of LLMs including Meta LLaMA, Qwen, and more.
- **Scalable Performance:** Built on Nebula Block's GPU-accelerated infrastructure.
- **Customizable Configurations:** Tailor endpoints to meet your workload requirements.

## Prerequisites
- **Nebula Block Account:** Ensure you have an account on the [Nebula Block](https://nebula-block.com).
- **Credit Balance:** Ensure you have at least $0.01 credit balance in your account.

## Pricing and Billing
- **Pay-As-You-Go:** Charges are based on token usage.
- Check the [pricing page](https://nebula-block.com/pricing) for details.